{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Ensemble models from machine learning: an example of wave runup and coastal dune erosion</center>\n",
    "### <center>Tomas Beuzen<sup>1</sup>, Evan B. Goldstein<sup>2</sup>, Kristen D. Splinter<sup>1</sup></center>\n",
    "<center><sup>1</sup>Water Research Laboratory, School of Civil and Environmental Engineering, UNSW Sydney, NSW, Australia</center>\n",
    "\n",
    "<center><sup>2</sup>Department of Geography, Environment, and Sustainability, University of North Carolina at Greensboro, Greensboro, NC, USA</center>\n",
    "\n",
    "\n",
    "This notebook contains the code required to reproduce the analysis and figures presented in the manuscript \"*Ensemble models from machine learning: an example of wave runup and coastal dune erosion*\" by Beuzen et al.\n",
    "\n",
    "**Citation:** Beuzen, T, Goldstein, E.B., Splinter, K.S. (In Review). Ensemble models from machine learning: an example of wave runup and coastal dune erosion, Natural Hazards and Earth Systems Science, SI Advances in computational modeling of geoprocesses and geohazards.\n",
    "\n",
    "### Table of Contents:\n",
    "* [Imports](#bullet-0)\n",
    "* [Figure 3](#bullet-1)\n",
    "* [Figure 4](#bullet-2)\n",
    "* [Figure 5](#bullet-3)\n",
    "* [Figure 7](#bullet-4)\n",
    "* [Figure 8](#bullet-5)\n",
    "* [Figure 9](#bullet-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"bullet-0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "# Standard computing packages\n",
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Gaussian Process tools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "# Custom functions\n",
    "from support_functions import train_test_mda, LEH04ensembles\n",
    "\n",
    "# Notebook functionality\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3 <a class=\"anchor\" id=\"bullet-1\"></a>\n",
    "In this section, we will load and visualise the wave, beach slope, and runup data we will use to develop the Gaussian process (GP) runup predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in .csv data file as a pandas dataframe\n",
    "df = pd.read_csv('../data_repo_temporary/lidar_runup_data.csv',parse_dates=True,index_col=0,dayfirst=True).dropna()\n",
    "# Print the size and head of the dataframe\n",
    "print('Data size:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell plots Figure 3, showing histograms of the data\n",
    "# Initialize the figure and axes\n",
    "fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "plt.tight_layout(w_pad=0.1, h_pad=3)\n",
    "# Subplot (0,0): Hs\n",
    "ax = axes[0,0]\n",
    "ax.hist(df.Hs,37,color=(0.6,0.6,0.6),edgecolor='k',lw=0.5) # Plot histogram\n",
    "ax.set_xlabel('H$_s$ (m)',fontweight='bold') # Format plot\n",
    "ax.set_ylabel('Frequency',fontweight='bold')\n",
    "ax.set_xticks((0,1.5,3,4.5))\n",
    "ax.set_xlim((0,4.5))\n",
    "ax.set_ylim((0,1200))\n",
    "ax.grid(lw=0.5,alpha=0.7)\n",
    "ax.text(-1.1, 1250, 'A)', fontweight='bold', fontsize=12)\n",
    "ax.tick_params(direction='in')\n",
    "ax.set_axisbelow(True)\n",
    "# Subplot (0,1): Tp\n",
    "ax = axes[0,1]\n",
    "ax.hist(df.Tp,20,color=(0.6,0.6,0.6),edgecolor='k',lw=0.5) # Plot histogram\n",
    "ax.set_xlabel('T$_p$ (s)',fontweight='bold') # Format plot\n",
    "ax.set_xticks((0,6,12,18))\n",
    "ax.set_xlim((0,18))\n",
    "ax.set_ylim((0,1200))\n",
    "ax.set_yticklabels([])\n",
    "ax.grid(lw=0.5,alpha=0.7)\n",
    "ax.text(-2.1, 1250, 'B)', fontweight='bold', fontsize=12)\n",
    "ax.tick_params(direction='in')\n",
    "ax.set_axisbelow(True)\n",
    "# Subplot (1,0): beta\n",
    "ax = axes[1,0]\n",
    "ax.hist(df.beach_slope,20,color=(0.6,0.6,0.6),edgecolor='k',lw=0.5) # Plot histogram\n",
    "ax.set_xlabel(r'$\\beta$',fontweight='bold') # Format plot\n",
    "ax.set_ylabel('Frequency',fontweight='bold')\n",
    "ax.set_xticks((0,0.1,0.2,0.3))\n",
    "ax.set_xlim((0,0.3))\n",
    "ax.set_ylim((0,1200))\n",
    "ax.grid(lw=0.5,alpha=0.7)\n",
    "ax.text(-0.073, 1250, 'C)', fontweight='bold', fontsize=12)\n",
    "ax.tick_params(direction='in')\n",
    "ax.set_axisbelow(True)\n",
    "# Subplot (1,1): R2\n",
    "ax = axes[1,1]\n",
    "ax.hist(df.runup,25,color=(0.9,0.2,0.2),edgecolor='k',lw=0.5) # Plot histogram\n",
    "ax.set_xlabel('R$_2$ (m)') # Format plot\n",
    "ax.set_xticks((0,1,2,3))\n",
    "ax.set_xlim((0,3))\n",
    "ax.set_ylim((0,1200))\n",
    "ax.set_yticklabels([])\n",
    "ax.grid(lw=0.5,alpha=0.7)\n",
    "ax.text(-0.35, 1250, 'D)', fontweight='bold', fontsize=12)\n",
    "ax.tick_params(direction='in')\n",
    "ax.set_axisbelow(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4 <a class=\"anchor\" id=\"bullet-2\"></a>\n",
    "In this section we will first split our data into training, validation and testing sets, before developing and evaluating the performance of the GP predictor.\n",
    "\n",
    "\n",
    "The training dataset is selected using a Maximum Dissimilarity Algorithm (MDA). The validation and testing sets are then selected by equally, randomly splitting all the data not selected as part of the trianing dataset. Extensive preliminary testing showed that a training dataset size of 5% was optimum for maximizing GP performance and minimizing computational complexity. See **Section 3.2** of the manuscript for further discussion.\n",
    "\n",
    "We then standardize the data for use in the GP by removing the mean and scaling to unit variance. This does not really affect GP performance but improves computational efficiency (see sklearn documentation for more information). Note that the scaling regime is only based on the training dataset, this scaling is then applied to the validation and testing datasets separately.\n",
    "\n",
    "A kenerl must be specified to develop the GP. Many kernels were trialled in initial GP development. The final kernel is a combination of the RBF and WhiteKernel. See **Section 2.1** and **Section 2.2** of the manuscript for further discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training/testing sets\n",
    "X = df.drop(columns=df.columns[-1]) # Drop the last column to retain input features\n",
    "y = df[[df.columns[-1]]]            # The last column is the predictand\n",
    "# Select the training dataset using Maximum Dissimilarity Algorithm (MDA).\n",
    "X_train, X_temp, y_train, y_temp = train_test_mda(X,\n",
    "                                                  y,\n",
    "                                                  sample_size=0.05,\n",
    "                                                  dist_measure='euclidean',\n",
    "                                                  standardize = True)\n",
    "# Now, equally split remaining data into validation and test datasets\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp,\n",
    "                                                              y_temp,\n",
    "                                                              test_size=0.5,\n",
    "                                                              shuffle=False,\n",
    "                                                              random_state=123)\n",
    "# Print dataset characteristics\n",
    "print('\\033[4mDATASETS\\033[0m')\n",
    "print('Training Size:   ' + str(len(X_train)))\n",
    "print('Validation Size: ' + str(len(X_validation)))\n",
    "print('Testing Size:    ' + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data for use in the GP\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # Fit the scaler to the training data\n",
    "X_train_scaled = scaler.transform(X_train) # Scale training data\n",
    "X_validation_scaled = scaler.transform(X_validation) # Scale validation data\n",
    "X_test_scaled = scaler.transform(X_test) # Scale testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the kernel to use in the GP\n",
    "kernel = RBF(0.1, (1e-2, 1e2)) + WhiteKernel(1,(1e-2,1e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GP model on training dataset\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, normalize_y=True, random_state=123)\n",
    "gp.fit(X_train_scaled, y_train)\n",
    "# Evaluate performance on the validation dataset\n",
    "y_validation_predictions, sigma_validation = gp.predict(X_validation_scaled, return_std=True)\n",
    "print('GP RMSE on validation data =', format(np.sqrt(mean_squared_error(y_validation,y_validation_predictions)),'.2f'), 'm')\n",
    "# Evaluate performance on the testing dataset\n",
    "y_test_predictions, sigma_test = gp.predict(X_test_scaled, return_std=True)\n",
    "print('GP RMSE on test data =', format(np.sqrt(mean_squared_error(y_test,y_test_predictions)),'.2f'), 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell plots Figure 4, comparing GP predictions to observations for the testing dataset\n",
    "# Initialize the figure and axes\n",
    "fig, axes = plt.subplots(figsize=(6,6))\n",
    "plt.tight_layout(pad=2.2)\n",
    "# Plot and format\n",
    "axes.scatter(y_test,y_test_predictions,s=10,c='b',marker='.')\n",
    "axes.plot([0,4],[0,4],'k--')\n",
    "axes.set_ylabel('Predicted R$_2$ (m)')\n",
    "axes.set_xlabel('Observed R$_2$ (m)')\n",
    "axes.grid(lw=0.5,alpha=0.7)\n",
    "axes.set_xlim(0,2.5)\n",
    "axes.set_ylim(0,2.5)\n",
    "# Print some statistics\n",
    "print('GP RMSE on test data =', format(np.sqrt(mean_squared_error(y_test,y_test_predictions)),'.2f'))\n",
    "print('GP bias on test data =', format(np.mean(y_test_predictions-y_test.values),'.2f'))\n",
    "print('correlation-squared =', format(pearsonr(y_test_predictions,y_test.values)[0][0]**2,'.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5 <a class=\"anchor\" id=\"bullet-3\"></a>\n",
    "This section explores how we can draw random samples from the GP to explain scatter in the runup predictions. We randomly draw 100 samples from the GP and calculate how much of the scatter in the runup predictions is captured by the ensemble envelope for different ensemble sizes. The process is repeated 100 times for robustness. See **Section 3.3** of the manuscript for further discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 100 samples from the GP model using the testing dataset\n",
    "GP_draws = gp.sample_y(X_test_scaled, n_samples=100, random_state=123).squeeze() # Draw 100 random samples from the GP\n",
    "# Initialize result arrays\n",
    "perc_ens = np.zeros((100,100)) # Initialize ensemble capture array\n",
    "perc_err = np.zeros((100,))   # Initialise arbitray error array\n",
    "# Loop to get results\n",
    "for i in range(0,perc_ens.shape[0]):\n",
    "    # Caclulate capture % in envelope created by adding arbitrary, uniform error to mean GP prediction\n",
    "    lower = y_test_predictions*(1-i/100) # Lower bound\n",
    "    upper = y_test_predictions*(1+i/100) # Upper bound\n",
    "    perc_err[i] = sum((np.squeeze(y_test)>=np.squeeze(lower)) & (np.squeeze(y_test)<=np.squeeze(upper)))/y_test.shape[0] # Store percent capture\n",
    "    for j in range(0,perc_ens.shape[1]):\n",
    "        ind = np.random.randint(0,perc_ens.shape[0],i+1) # Determine i random integers\n",
    "        lower = np.min(GP_draws[:,ind],axis=1) # Lower bound of ensemble of i random members\n",
    "        upper = np.max(GP_draws[:,ind],axis=1) # Upper bound of ensemble of i random members\n",
    "        perc_ens[i,j] = sum((np.squeeze(y_test)>=lower) & (np.squeeze(y_test)<=upper))/y_test.shape[0] # Store percent capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell plots Figure 5, showing how samples from the GP can help to capture uncertainty in predictions\n",
    "# Initialize the figure and axes\n",
    "fig, axes = plt.subplots(1,2,figsize=(9,4))\n",
    "plt.tight_layout()\n",
    "lim = 0.95 # Desired limit to test\n",
    "# Plot ensemble results\n",
    "ax = axes[0]\n",
    "perc_ens_mean = np.mean(perc_ens,axis=1)\n",
    "ax.plot(perc_ens_mean*100,'k-',lw=2)\n",
    "ind = np.argmin(abs(perc_ens_mean-lim)) # Find where the capture rate > lim\n",
    "ax.plot([ind,ind],[0,perc_ens_mean[ind]*100],'r--')\n",
    "ax.plot([0,ind],[perc_ens_mean[ind]*100,perc_ens_mean[ind]*100],'r--')\n",
    "ax.set_xlabel('# Draws from GP')\n",
    "ax.set_ylabel('Observations captured \\n within ensemble range (%)')\n",
    "ax.grid(lw=0.5,alpha=0.7)\n",
    "ax.minorticks_on()\n",
    "ax.set_xlim(0,100);\n",
    "ax.set_ylim(0,100);\n",
    "ax.text(-11.5, 107, 'A)', fontweight='bold', fontsize=12)\n",
    "print('# draws needed for ' + format(lim*100,'.0f') + '% capture = ' + str(ind))\n",
    "print('Mean/Min/Max for ' + str(ind) + ' draws = '\n",
    "      + format(np.mean(perc_ens[ind,:])*100,'.1f') + '%/'\n",
    "      + format(np.min(perc_ens[ind,:])*100,'.1f') + '%/'\n",
    "      + format(np.max(perc_ens[ind,:])*100,'.1f') + '%')\n",
    "# Plot arbitrary error results\n",
    "ax = axes[1]\n",
    "ax.plot(perc_err*100,'k-',lw=2)\n",
    "ind = np.argmin(abs(perc_err-lim))  # Find where the capture rate > lim\n",
    "ax.plot([ind,ind],[0,perc_err[ind]*100],'r--')\n",
    "ax.plot([0,ind],[perc_err[ind]*100,perc_err[ind]*100],'r--')\n",
    "ax.set_xlabel('% Error added to mean GP estimate')\n",
    "ax.grid(lw=0.5,alpha=0.7)\n",
    "ax.minorticks_on()\n",
    "ax.set_xlim(0,100);\n",
    "ax.set_ylim(0,100);\n",
    "ax.text(-11.5, 107, 'B)', fontweight='bold', fontsize=12)\n",
    "print('% added error needed for ' + format(lim*100,'.0f') + '% capture = ' + str(ind) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7 <a class=\"anchor\" id=\"bullet-4\"></a>\n",
    "We now show how the GP predictor can be used within a dune erosion model to form ensemble predictions of dune erosion. We will import data of 40 profiles from a storm event that impacted Narrabeen Beach, Sydney, Australia in June 2011.  See **Section 4.2** of the manuscript for further information on this dataset. We use the model of Larson et al (2004) (LEH04) with the GP runup predictor to form model ensembles as discussed in **Section 4.1** of the manuscript.\n",
    "\n",
    "\n",
    "10,000 runup models are drawn from the GP runup predictor and used within the LEH04 model to create an ensemble of dune erosion predictions. This section shows one such prediction for a single profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in beach slope, wave and water level data for the 40 profiles impacted by the June 2011 storm and make predictions\n",
    "# of runup using the GP\n",
    "draws = 10000 # We will draw 10,000 ensembles\n",
    "june2011_waves = pd.read_csv('../data_repo_temporary/june2011_slope_wave_waterlevel_data.csv',index_col=0) # Read the data\n",
    "water_level = june2011_waves.loc[i+1]['water_level'] # Isolate the water level to be added to R2 predictions\n",
    "# Initialize result arrays\n",
    "GP_mean = np.zeros((40,120))\n",
    "GP_sigma = np.zeros((40,120))\n",
    "GP_samples = np.zeros((40,120,draws))\n",
    "# Loop to predict wave runup at each profile\n",
    "for i in range(40):\n",
    "    X = scaler.transform(june2011_waves.loc[i+1].drop(columns='water_level'))\n",
    "    y, sigma = gp.predict(X, return_std=True) # Predict R2\n",
    "    GP_mean[i,:] = np.squeeze(y) + water_level # add the water level to R2 predictions\n",
    "    GP_sigma[i,:] = sigma\n",
    "    y_samples = gp.sample_y(X, n_samples=draws).squeeze() + water_level[:, None]  # sample draws from the GP R2 predictor\n",
    "    GP_samples[i,:,:] = y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the morphological data for the 40 profiles, including dune toe position, dune erosion volumes, etc.\n",
    "with open('../data_repo_temporary/june2011_profile_data.pkl', 'rb') as f:\n",
    "    june2011_profiles = pickle.load(f) # load the data\n",
    "# Define some parameters\n",
    "profile_index = 12; # a profile to analyse\n",
    "Cs = 0.0015 # Cs value to use in LEH04\n",
    "# Pull required data out of the june2011_profiles dictionary\n",
    "profile = data[profile_index] # get the profile data  \n",
    "dv = profile['dv'] # dune volume\n",
    "zb = profile['zb'] # dune toe\n",
    "T = profile['Tp'] # wave period\n",
    "GP_sample = np.squeeze(GP_samples[profile_index,:,:]) # the GP runup draws for the profile\n",
    "# Run the LEH04 model\n",
    "[SigDuneErosionGPD,zbmGPD] = LEH04ensembles(dv,zb,GP_sample,T,Cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell plots Figure 7, showing an example of the GP runup predictor used with the LEH04 model to produce ensemble\n",
    "# predictions of dune erosion\n",
    "# Initialize the figure and axes\n",
    "fig, _ = plt.subplots(figsize=(19/2.54,17/2.54))\n",
    "plt.tight_layout(pad=0.1,w_pad=0.1,h_pad=0.1)\n",
    "ax1 = plt.subplot2grid((5, 1), (0, 0), rowspan=3)\n",
    "ax2 = plt.subplot2grid((5, 1), (3, 0), rowspan=2)\n",
    "# Define some parameters\n",
    "t = np.arange(1, 121) # time\n",
    "ens = 10000 # number of ensemble members to plot\n",
    "profile['times']=[1,24,47,74,119]; # observed dune characteristics at times through the storm event\n",
    "profile['zb_12345']=[2.6310,2.8427,2.8849,2.9939,3.0162]\n",
    "profile['dv_12345']=[0.1301,0.7682,1.2730,1.8308,2.2395]\n",
    "# Observed Dune Toe Position\n",
    "ax1.plot(profile['times'],profile['zb_12345'],'.',markersize=15,mfc='pink',mew=1,mec='k',zorder=10,\n",
    "         label='Observed Dune Toe Position')\n",
    "# Runup\n",
    "ax1.plot([],'w',label=' ')\n",
    "ax1.plot([],'w',label='Runup Predictions:')\n",
    "ax1.plot(t, np.median(GP_sample[:,:ens],axis=1),'-b',lw=2,label='Ensemble Mean',zorder=10)\n",
    "ax1.fill_between(t,np.percentile(GP_sample[:,:ens-1],17,axis=1),\n",
    "                  np.percentile(GP_sample[:,:ens-1],83,axis=1),\n",
    "                  alpha=0.4,\n",
    "                  facecolor='royalblue',\n",
    "                  label='66% Range',zorder=10)\n",
    "ax1.fill_between(t,np.percentile(GP_sample[:,:ens-1],5,axis=1),\n",
    "                  np.percentile(GP_sample[:,:ens-1],95,axis=1),\n",
    "                  alpha=0.3,\n",
    "                  facecolor='royalblue',\n",
    "                  label='90% Range',zorder=10)\n",
    "ax1.fill_between(t,np.percentile(GP_sample[:,:ens-1],0.5,axis=1),\n",
    "                  np.percentile(GP_sample[:,:ens-1],99.5,axis=1),\n",
    "                  alpha=0.2,\n",
    "                  facecolor='royalblue',\n",
    "                  label='99% Range',zorder=10)\n",
    "# Modelled Dune Toe Position\n",
    "ax1.plot([],'w',label=' ')\n",
    "ax1.plot([],'w',label='Dune Toe Predictions:')\n",
    "ax1.plot(t, np.mean(zbmGPD[:,:ens],axis=1),'-k',lw=3,label='Ensemble Mean',zorder=1)\n",
    "ax1.fill_between(t,np.percentile(zbmGPD[:,:ens-1],17,axis=1),\n",
    "                  np.percentile(zbmGPD[:,:ens-1],83,axis=1),\n",
    "                  alpha=0.9,\n",
    "                  facecolor='grey',\n",
    "                  label='66% Range')\n",
    "ax1.fill_between(t,np.percentile(zbmGPD[:,:ens-1],5,axis=1),\n",
    "                  np.percentile(zbmGPD[:,:ens-1],95,axis=1),\n",
    "                  alpha=0.5,\n",
    "                  facecolor='grey',\n",
    "                  label='90% Range')\n",
    "ax1.fill_between(t,np.percentile(zbmGPD[:,:ens-1],0.5,axis=1),\n",
    "                  np.percentile(zbmGPD[:,:ens-1],99.5,axis=1),\n",
    "                  alpha=0.3,\n",
    "                  facecolor='grey',\n",
    "                  label='99% Range')\n",
    "# Formatting\n",
    "ax1.set_title('Profile 141')\n",
    "ax1.set_ylabel('Elevation (m)')\n",
    "ax1.set_yticks(np.linspace(0,4,5))\n",
    "ax1.set_ylim(0,4)\n",
    "ax1.set_xlim(0,120)\n",
    "ax1.set_xticklabels([])\n",
    "ax1.legend()\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "handles = [handles[i] for i in [0,1,2,3,7,8,9,4,5,6,10,11,12]]\n",
    "labels = [labels[i] for i in [0,1,2,3,7,8,9,4,5,6,10,11,12]]\n",
    "ax1.legend(handles,labels,loc=6,bbox_to_anchor=(1, 0.5))\n",
    "ax1.text(-9.4, 3.9, 'A)', fontweight='bold', fontsize=12)\n",
    "# Observed Dune Erosion\n",
    "ax2.plot(profile['times'],profile['dv_12345'],'.',markersize=15,mfc='pink',mew=1,mec='k',zorder=10,\n",
    "         label='Observed Dune Erosion')\n",
    "# Modelled Dune Toe Position\n",
    "ax2.plot(t, np.mean(SigDuneErosionGPD[:,:ens],axis=1),'-k',lw=3,label='Ensemble Mean',zorder=9)\n",
    "ax2.plot([],'w',label=' ')\n",
    "ax2.plot([],'w',label='Dune Erosion Predictions:    ')\n",
    "ax2.fill_between(t,np.percentile(SigDuneErosionGPD[:,:ens-1],17,axis=1),\n",
    "                  np.percentile(SigDuneErosionGPD[:,:ens-1],83,axis=1),\n",
    "                  alpha=0.9,\n",
    "                  facecolor='grey',\n",
    "                  label='66% Range')\n",
    "ax2.fill_between(t,np.percentile(SigDuneErosionGPD[:,:ens-1],5,axis=1),\n",
    "                  np.percentile(SigDuneErosionGPD[:,:ens-1],95,axis=1),\n",
    "                  alpha=0.5,\n",
    "                  facecolor='grey',\n",
    "                  label='90% Range')\n",
    "ax2.fill_between(t,np.percentile(SigDuneErosionGPD[:,:ens-1],0.5,axis=1),\n",
    "                  np.percentile(SigDuneErosionGPD[:,:ens-1],99.5,axis=1),\n",
    "                  alpha=0.3,\n",
    "                  facecolor='grey',\n",
    "                  label='99% Range')\n",
    "# Formatting\n",
    "ax2.set_ylabel('Dune Erosion (m$^3$/m)')\n",
    "ax2.set_xlabel('Time (hours)')\n",
    "ax2.set_yticks(np.linspace(0,8,5))\n",
    "ax2.set_ylim(0,8)\n",
    "ax2.set_xlim(0,120)\n",
    "ax2.legend()\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "handles = [handles[i] for i in [0,2,3,1,4,5,6]]\n",
    "labels = [labels[i] for i in [0,2,3,1,4,5,6]]\n",
    "ax2.legend(handles,labels,loc=6,bbox_to_anchor=(1, 0.5))\n",
    "ax2.text(-9.4, 7.8, 'B)', fontweight='bold', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 8 <a class=\"anchor\" id=\"bullet-5\"></a>\n",
    "Figure 7 previously showed how the GP predictor + LEH04 model can be used to geenrate ensemble dune erosion predictions. We will now show the performance of the approach on all 40 profiles. Effectively the same code as previously is applied again, but over all 40 profiles and the results are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "Cs = 0.0015 # Cs to use in LEH04\n",
    "draws = 10000 # # We will use 10,000 ensembles\n",
    "# Initialize result arrays\n",
    "obs = np.zeros(len(data))\n",
    "MeanGP = np.zeros(len(data))\n",
    "MedianGP = np.zeros(len(data))\n",
    "MaxGP66 = np.zeros(len(data))\n",
    "MinGP66 = np.zeros(len(data))\n",
    "MaxGP90 = np.zeros(len(data))\n",
    "MinGP90 = np.zeros(len(data))\n",
    "MaxGP99 = np.zeros(len(data))\n",
    "MinGP99 = np.zeros(len(data))\n",
    "errorGPmean = np.zeros(len(data))\n",
    "errorGPmedian = np.zeros(len(data))\n",
    "#loop through each profile and make dune erosion predictions\n",
    "for k in june2011_profiles:\n",
    "    profile=june2011_profiles[k] # get the profiel data\n",
    "    obs[k] = june2011_profiles[k]['dv_obs'] # store observed dune erosion           \n",
    "    # pull all relevant data out of the dictionary\n",
    "    dv = profile['dv']\n",
    "    zb = profile['zb']\n",
    "    T = profile['Tp']\n",
    "    GP_sample = np.squeeze(GP_samples[k,:,:]) # the GP runup draws for the profile\n",
    "    # run the LEH04 model\n",
    "    [SigDuneErosionGPD,zbmGPD] = LEH04ensembles(dv,zb,GP_sample,T,Cs)\n",
    "    # record the max and min from GP draws for different probabilities\n",
    "    MeanGP[k] = np.mean(SigDuneErosionGPD[-1,:])\n",
    "    MedianGP[k] = np.median(SigDuneErosionGPD[-1,:])\n",
    "    MaxGP66[k] = np.percentile(SigDuneErosionGPD[-1,:],83)\n",
    "    MinGP66[k] = np.percentile(SigDuneErosionGPD[-1,:],17)\n",
    "    MaxGP90[k] = np.percentile(SigDuneErosionGPD[-1,:],95)\n",
    "    MinGP90[k] = np.percentile(SigDuneErosionGPD[-1,:],5)\n",
    "    MaxGP99[k] = np.percentile(SigDuneErosionGPD[-1,:],99.5)\n",
    "    MinGP99[k] = np.percentile(SigDuneErosionGPD[-1,:],0.5)\n",
    "    #GP ensemble mean and median\n",
    "    errorGPmean[k] = np.absolute(profile['dv_obs'] - MeanGP[k])\n",
    "    errorGPmedian[k] = np.absolute(profile['dv_obs'] - MedianGP[k])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell plots Figure 8, showing predictions for all 40 profiles\n",
    "# Initialize the figure and axes\n",
    "fig, axes = plt.subplots(1,1,figsize=(10,6))\n",
    "# Plot\n",
    "x = np.arange(1, len(obs)+1)\n",
    "axes.plot(x,obs,'o',linestyle='-',color='pink',mfc='pink',mew=1,mec='k',zorder=10,label='Observed')\n",
    "axes.plot(x,MeanGP,'k',marker='o',label='Ensemble Mean')\n",
    "\n",
    "axes.fill_between(x,\n",
    "                  MinGP66,\n",
    "                  MaxGP66,\n",
    "                  alpha=0.9,\n",
    "                  facecolor='grey',\n",
    "                  label='66% Range')\n",
    "axes.fill_between(x,\n",
    "                  MinGP90,\n",
    "                  MaxGP90,\n",
    "                  alpha=0.5,\n",
    "                  facecolor='grey',\n",
    "                  label='90% Range')\n",
    "axes.fill_between(x,\n",
    "                  MinGP99,\n",
    "                  MaxGP99,\n",
    "                  alpha=0.3,\n",
    "                  facecolor='grey',\n",
    "                  label='99% Range')\n",
    "# Formatting\n",
    "axes.set_xlim(0,41)\n",
    "axes.set_ylim(0,20)\n",
    "axes.set_xlabel('Profile')\n",
    "axes.set_ylabel('Dune erosion (m$^3$/m)')\n",
    "axes.grid()\n",
    "axes.legend(framealpha=1)\n",
    "# Calcualte and print r-squared\n",
    "r_squared = pearsonr(obs,MeanGP)[0]**2\n",
    "print('r2 =', format(r_squared,'.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 9 <a class=\"anchor\" id=\"bullet-6\"></a>\n",
    "In the previous code and figures we have fixed the Cs parameter of the LEH04 model to be 1.5x10-3 and the number of ensembles to be 10,000. We will now expore the senstivity of the GP + LEH04 approach to variations in these two parameters. The code below is effectively the same as that used above except iterated over many Cs values. ***Note that this code block can take several hours to run.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter space\n",
    "Cs = np.logspace(-5,-1,50);\n",
    "draws=10000\n",
    "# Initialize result arrays\n",
    "errorGP = np.zeros((len(june2011_profiles),len(Cs)))\n",
    "errorST = np.zeros((len(june2011_profiles),len(Cs)))\n",
    "errorGPmean = np.zeros((len(june2011_profiles),len(Cs),draws))\n",
    "MaxGP = np.zeros((len(june2011_profiles),len(Cs),draws));\n",
    "MinGP = np.zeros((len(june2011_profiles),len(Cs),draws));\n",
    "MaxGP66 = np.zeros((len(june2011_profiles),len(Cs),draws));\n",
    "MinGP66 = np.zeros((len(june2011_profiles),len(Cs),draws));\n",
    "MaxGP90 = np.zeros((len(june2011_profiles),len(Cs),draws));\n",
    "MinGP90 = np.zeros((len(june2011_profiles),len(Cs),draws));\n",
    "MaxGP99 = np.zeros((len(june2011_profiles),len(Cs),draws));\n",
    "MinGP99 = np.zeros((len(june2011_profiles),len(Cs),draws));\n",
    "MeanGP = np.zeros((len(june2011_profiles),len(Cs),draws));\n",
    "# loop through all the Cs indicies\n",
    "for j in range(len(Cs)):\n",
    "    # loop through the profile indicies\n",
    "    for k in june2011_profiles:\n",
    "        profile=june2011_profiles[k]\n",
    "        #pull all relevant data out of the dictionary\n",
    "        dv = profile['dv']\n",
    "        zb = profile['zb']\n",
    "        T = profile['Tp']\n",
    "        GP_sample = np.squeeze(GP_samples[k,:,:]) # the GP runup draws for the profile\n",
    "        # run the LEH04 model\n",
    "        [SigDuneErosionGPD,zbmGPD] = LEH04ensembles(dv,zb,GP_sample,T,Cs[j])\n",
    "        # record some results\n",
    "        for n in range(draws):\n",
    "            MeanGP[k,j,n] = np.mean(SigDuneErosionGPD[-1,0:n+1])\n",
    "            MaxGP66[k,j,n] = np.percentile(SigDuneErosionGPD[-1,0:n+1],83)\n",
    "            MinGP66[k,j,n] = np.percentile(SigDuneErosionGPD[-1,0:n+1],17)\n",
    "            MaxGP90[k,j,n] = np.percentile(SigDuneErosionGPD[-1,0:n+1],95)\n",
    "            MinGP90[k,j,n] = np.percentile(SigDuneErosionGPD[-1,0:n+1],5)\n",
    "            MaxGP99[k,j,n] = np.percentile(SigDuneErosionGPD[-1,0:n+1],99.5)\n",
    "            MinGP99[k,j,n] = np.percentile(SigDuneErosionGPD[-1,0:n+1],0.5)\n",
    "    \n",
    "        errorGPmean[k,j,:] = np.absolute(profile['dv_obs'] - MeanGP[k,j,:])\n",
    "# record the capture statistics\n",
    "PercentWithin66 = np.zeros((len(Cs),draws))\n",
    "PercentWithin90 = np.zeros((len(Cs),draws))\n",
    "PercentWithin99 = np.zeros((len(Cs),draws))\n",
    "obs = np.zeros((len(june2011_profiles)))\n",
    "for h in june2011_profiles:\n",
    "    obs[h] = june2011_profiles[h]['dv_obs']  \n",
    "for f in range(len(Cs)):\n",
    "    for g in range(draws):\n",
    "        within = np.where((MaxGP66[:,f,g]>=obs) & (MinGP66[:,f,g]<=obs))\n",
    "        PercentWithin66[f,g] = 100*(len(within[0])/(len(june2011_profiles)));\n",
    "        \n",
    "        within = np.where((MaxGP90[:,f,g]>=obs) & (MinGP90[:,f,g]<=obs))\n",
    "        PercentWithin90[f,g] = 100*(len(within[0])/(len(june2011_profiles)));\n",
    "        \n",
    "        within = np.where((MaxGP99[:,f,g]>=obs) & (MinGP99[:,f,g]<=obs))\n",
    "        PercentWithin99[f,g] = 100*(len(within[0])/(len(june2011_profiles)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure and axes\n",
    "fig, axes = plt.subplots(nrows=4,ncols=1,figsize=(12/2.54,24/2.54))\n",
    "plt.tight_layout(w_pad = 4, h_pad = 1)\n",
    "# Plot MAE results\n",
    "lineObjects2 = axes[0].semilogx(Cs,np.mean(errorGPmean[:,:,[4,9,19,99,999,9999]], axis=0))\n",
    "axes[0].set_ylabel('MAE for all profiles (m$^3$/m)')\n",
    "axes[0].set_ylim(0, 10) \n",
    "axes[0].set_xlim(10**-5,10**-1)\n",
    "axes[0].grid(which='both')\n",
    "axes[0].legend(lineObjects2,\n",
    "               ('5','10','20','100','1000','10,000'),\n",
    "               title='# Ensemble\\n    Members',\n",
    "               bbox_to_anchor=(0,0.5),\n",
    "               loc=6,\n",
    "               framealpha=1)\n",
    "axes[0].text(7.2**-7, 10.4, 'A)', fontweight='bold', fontsize=12)\n",
    "# Plot 99% capture results\n",
    "axes[1].semilogx(Cs, PercentWithin99[:,[4,9,19,99,999,9999]])\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].set_xlim(10**-5,10**-1)\n",
    "axes[1].set_ylabel('% of dune erosion \\ndata within 99% range')\n",
    "axes[1].set_xlim(10**-5,10**-1)\n",
    "axes[1].grid(which='both') \n",
    "axes[1].text(7.2**-7, 104, 'B)', fontweight='bold', fontsize=12)\n",
    "# Plot 95% capture results\n",
    "axes[2].semilogx(Cs, PercentWithin90[:,[4,9,19,99,999,9999]])\n",
    "axes[2].set_ylim(0, 100) \n",
    "axes[2].set_ylabel('% of dune erosion \\ndata within 90% range')\n",
    "axes[2].set_xlim(10**-5,10**-1)\n",
    "axes[2].grid(which='both') \n",
    "axes[2].text(7.2**-7, 104, 'C)', fontweight='bold', fontsize=12)\n",
    "# Plot 66% capture results\n",
    "axes[3].semilogx(Cs, PercentWithin66[:,[4,9,19,99,999,9999]])\n",
    "axes[3].set_xlabel('Cs')\n",
    "axes[3].set_ylim(0, 100) \n",
    "axes[3].set_ylabel('% of dune erosion \\ndata within 66% range')\n",
    "axes[3].set_xlim(10**-5,10**-1)\n",
    "axes[3].grid(which='both') \n",
    "axes[3].text(7.2**-7, 104, 'D)', fontweight='bold', fontsize=12)\n",
    "# Print out some useful stats\n",
    "Cs_5_ind = np.argmin(np.mean(errorGPmean[:,:,4],axis=0))\n",
    "Cs_10_ind = np.argmin(np.mean(errorGPmean[:,:,9],axis=0))\n",
    "Cs_20_ind = np.argmin(np.mean(errorGPmean[:,:,19],axis=0))\n",
    "Cs_100_ind = np.argmin(np.mean(errorGPmean[:,:,99],axis=0))\n",
    "Cs_1000_ind = np.argmin(np.mean(errorGPmean[:,:,999],axis=0))\n",
    "Cs_10000_ind = np.argmin(np.mean(errorGPmean[:,:,9999],axis=0))\n",
    "print('Ensemble Members | Optimum Cs')\n",
    "print('               5 : ' + format(Cs[Cs_5_ind],'.4f'))\n",
    "print('              10 : ' + format(Cs[Cs_10_ind],'.4f'))\n",
    "print('              20 : ' + format(Cs[Cs_20_ind],'.4f'))\n",
    "print('             100 : ' + format(Cs[Cs_100_ind],'.4f'))\n",
    "print('            1000 : ' + format(Cs[Cs_1000_ind],'.4f'))\n",
    "print('          10,000 : ' + format(Cs[Cs_10000_ind],'.4f'))\n",
    "print('')\n",
    "print('Ensemble Members | MAE')\n",
    "print('               5 : ' + format(np.mean(errorGPmean[:,Cs_5_ind,4],axis=0),'.2f'))\n",
    "print('              10 : ' + format(np.mean(errorGPmean[:,Cs_10_ind,9],axis=0),'.2f'))\n",
    "print('              20 : ' + format(np.mean(errorGPmean[:,Cs_20_ind,19],axis=0),'.2f'))\n",
    "print('             100 : ' + format(np.mean(errorGPmean[:,Cs_100_ind,99],axis=0),'.2f'))\n",
    "print('            1000 : ' + format(np.mean(errorGPmean[:,Cs_1000_ind,999],axis=0),'.2f'))\n",
    "print('          10,000 : ' + format(np.mean(errorGPmean[:,Cs_10000_ind,9999],axis=0),'.2f'))\n",
    "print('')\n",
    "print('Ensemble Members | 66% | 90% | 99%')\n",
    "print('               5 : ' + \n",
    "      format(PercentWithin66[Cs_5_ind,4],'.0f') + '    ' +\n",
    "      format(PercentWithin90[Cs_5_ind,4],'.0f') + '    ' +\n",
    "      format(PercentWithin99[Cs_5_ind,4],'.0f'))\n",
    "print('              10 : ' + \n",
    "      format(PercentWithin66[Cs_10_ind,9],'.0f') + '    ' +\n",
    "      format(PercentWithin90[Cs_10_ind,9],'.0f') + '    ' +\n",
    "      format(PercentWithin99[Cs_10_ind,9],'.0f'))\n",
    "print('              20 : ' + \n",
    "      format(PercentWithin66[Cs_20_ind,19],'.0f') + '    ' +\n",
    "      format(PercentWithin90[Cs_20_ind,19],'.0f') + '    ' +\n",
    "      format(PercentWithin99[Cs_20_ind,19],'.0f'))\n",
    "print('             100 : ' + \n",
    "      format(PercentWithin66[Cs_100_ind,99],'.0f') + '    ' +\n",
    "      format(PercentWithin90[Cs_100_ind,99],'.0f') + '    ' +\n",
    "      format(PercentWithin99[Cs_100_ind,99],'.0f'))\n",
    "print('            1000 : ' + \n",
    "      format(PercentWithin66[Cs_1000_ind,999],'.0f') + '    ' +\n",
    "      format(PercentWithin90[Cs_1000_ind,999],'.0f') + '    ' +\n",
    "      format(PercentWithin99[Cs_1000_ind,999],'.0f'))\n",
    "print('          10,000 : ' + \n",
    "      format(PercentWithin66[Cs_10000_ind,9999],'.0f') + '    ' +\n",
    "      format(PercentWithin90[Cs_10000_ind,9999],'.0f') + '    ' +\n",
    "      format(PercentWithin99[Cs_10000_ind,9999],'.0f'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
